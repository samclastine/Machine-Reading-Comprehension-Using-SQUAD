##################training_final.py##############################
# -*- coding: utf-8 -*-
"""training-final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LRW2SQT1lxk7I_Csne0Js3JcaHy-OYKH
"""

!pip install datasets

!pip install transformers

"""# Importing Libraries



"""

import pandas as pd
import numpy as np
import json
from collections import Counter
from datasets import load_dataset
import re
import spacy
import time
import matplotlib.pyplot as plt

import nltk
from nltk.util import ngrams
from nltk.tokenize import TreebankWordTokenizer, word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix , accuracy_score, roc_curve ,auc

from transformers import BertTokenizerFast, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, XLMRobertaForSequenceClassification,RobertaTokenizer, AdamW, get_linear_schedule_with_warmup,AutoTokenizer

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split


spacy.prefer_gpu()
nlp = spacy.load("en_core_web_sm")
nltk.download('punkt')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""# Data Loading and preprocessing """

train = load_dataset('squad_v2',split='train')

train_df = pd.DataFrame(train)
train_df

label=[]
for r in train_df['answers']:
  if r['text'] == []:
    label.append(1)
  else:
    label.append(0)

train_df['label']=label
train_df['label'].value_counts()

train_df['feature'] = train_df['context'] + ' ' + train_df['question']
train_df['feature'][0]

sample_df = train_df.sample(frac=1, random_state=42)  
sample_df = sample_df.head(5000)
sample_df = sample_df.reset_index(drop=True)

X_train, X_val, y_train, y_val = train_test_split(sample_df['feature'], sample_df['label'], test_size=0.2, random_state=42)

"""# Training Logistic Regression (BASELINE)"""

#https://stackoverflow.com/questions/53664775/how-to-remove-punctuation-in-python
sample_df['proccessed_context'] = sample_df['feature'].apply(lambda x: re.sub(r'http\S+', '', x))
# remove punctuation marks
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'
sample_df['proccessed_context']= sample_df['proccessed_context'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))
# convert text to lowercase
sample_df['proccessed_context'] = sample_df['proccessed_context'].str.lower()

tokenizer_tb = TreebankWordTokenizer()

sample_df['tokenized_context'] = sample_df['proccessed_context'].apply(lambda x : tokenizer_tb.tokenize(x))

#https://iconix.github.io/portfolio%20building/2017/09/25/nlp-for-tasks
#https://stackoverflow.com/questions/54849111/negation-and-dependency-parsing-with-spacy
def extract_pos(text):
    doc = nlp(text)
    pos_tags = [token.pos_ for token in doc]
    return pos_tags 


def extract_neg(text):
    doc = nlp(text)
    negations =np.zeros(len(doc))

    for i, token in enumerate(doc):
        if token.dep_ == "neg":
            # Check if negation applies to target word or its children
            target = None
            for child in token.children:
                if child.dep_ in ["acomp", "amod", "advcl", "advmod", "attr", "ccomp", "conj", "dobj", "iobj", "mark", "nsubj", "nsubjpass", "oprd", "xcomp"]:
                    target = child
                    break
            if target is None:
                target = token.head
            if target is not None:
                negations[target.i] = 1
                for child in target.children:
                    if child.dep_ in ["acomp", "amod", "advcl", "advmod", "attr", "ccomp", "conj", "dobj", "iobj", "mark", "nsubj", "nsubjpass", "oprd", "xcomp"]:
                        negations[child.i] = 1

    return negations

features_pos = sample_df['proccessed_context'].apply(lambda x : extract_pos(x))

features_neg = sample_df['proccessed_context'].apply(lambda x : extract_neg(x))

vectorizer = TfidfVectorizer()

_X_context = vectorizer.fit_transform([' '.join(tokens) for tokens in sample_df['tokenized_context']])

X_context = vectorizer.fit_transform(sample_df['proccessed_context'])

X_postag = vectorizer.fit_transform([' '.join(tokens) for tokens in features_pos])

sum_up= np.array(features_neg)

neg_sum = []
for val in sum_up:
  neg_sum.append(val.sum())

neg_array = np.array(neg_sum)

X_features = np.concatenate((_X_context.toarray(),X_postag.toarray(),neg_array.reshape(-1,1)),axis=1)
X_train, X_val, y_train, y_val = train_test_split(X_features, sample_df['label'], test_size=0.2, random_state=42)

lr = LogisticRegression()

lr.fit(X_train,y_train)

print('Train Score',lr.score(X_train,y_train))

print('Validation Score: ',lr.score(X_val,y_val))

X_val_set = np.array(X_val)
y_val_set = np.array(y_val)

np.save('/content/drive/MyDrive/NLP-COURSEWORK/dataset/X_test.npy',X_val_set)
np.save('/content/drive/MyDrive/NLP-COURSEWORK/dataset/y_test.npy',y_val_set)

import pickle

#https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/
filename = '/content/drive/MyDrive/NLP-COURSEWORK/models/baselinemodel.h5'
pickle.dump(lr, open(filename, 'wb'))

pred_lr = lr.predict(X_val)
print(classification_report(y_val, pred_lr,target_names=['answerable','unanswerable']))
#https://www.turing.com/kb/how-to-plot-confusion-matrix
# Calculate confusion matrix
cm01 = confusion_matrix(y_val, pred_lr)
# Plot confusion matrix
plt.imshow(cm01, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['answerable','unanswerable'], rotation=45)
plt.yticks(tick_marks, ['answerable','unanswerable'])
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Add numbers to the plot
thresh = cm01.max() / 2.
for i, j in np.ndindex(cm01.shape):
    plt.text(j, i, format(cm01[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm01[i, j] > thresh else "black")

plt.show()

"""# Training BERT-BASE"""

bert_base_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", do_lower_case=True,add_special_tokens=True)

train_encodings_bert = bert_base_tokenizer(X_train.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')
val_encodings_bert = bert_base_tokenizer(X_val.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt',)

input_ids_train_bert = torch.cat([train_encodings_bert['input_ids']], dim=0)
attention_mask_train_bert = torch.cat([train_encodings_bert['attention_mask']], dim=0)
labels_train = torch.tensor(y_train.to_list())

input_ids_val_bert = torch.cat([val_encodings_bert['input_ids']], dim=0)
attention_mask_val_bert = torch.cat([val_encodings_bert['attention_mask']], dim=0)
labels_val = torch.tensor(y_val.to_list())

train_tensor_bert = TensorDataset(input_ids_train_bert,attention_mask_train_bert,labels_train)
val_tensor_bert = TensorDataset(input_ids_val_bert,attention_mask_val_bert,labels_val)

# https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification
batch_size = 16
train_dataloader_bert = DataLoader(
            train_tensor_bert,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )
val_dataloader_bert = DataLoader(
            val_tensor_bert,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_bert = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
)

model_bert = model_bert.to(device)

optimizer_bert = AdamW(model_bert.parameters(),lr = 2e-5, eps = 1e-8 )

def validate(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0

  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

  accuracy = correct / total
  loss /= len(dataloader)
  return accuracy, loss

n_epoch = 10
best_val_loss = 0
for epoch in range(n_epoch):
  ts = time.time()

  for data in train_dataloader_bert:
    input_ids,attention_masks,labels = data

    _input_ids = input_ids.to(device)
    _attention_masks = attention_masks.to(device)
    _labels = labels.to(device)

    outputs = model_bert(_input_ids,token_type_ids=None,attention_mask=_attention_masks,labels=_labels)
    loss = outputs.loss
    loss.backward()
    optimizer_bert.step()
    optimizer_bert.zero_grad()
    train_loss = loss.item()

  val_accuracy, val_loss = validate(model_bert, val_dataloader_bert)
  te = time.time() - ts
  if val_loss > best_val_loss:
    best_val_loss = val_loss
    torch.save(model_bert.state_dict(), "/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_bert.pt")
  print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}, duration: {:.2f} s'.format(epoch+1, n_epoch, train_loss, val_loss, val_accuracy, te))

loaded_bert_model = model_bert.load_state_dict(torch.load("/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_bert.pt"))

"""# Training ROBERTA-Base"""

roberta_base_tokenizer = RobertaTokenizer.from_pretrained("roberta-base", do_lower_case=True,add_special_tokens=True)

train_encodings_roberta = roberta_base_tokenizer(X_train.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')
val_encodings_roberta = roberta_base_tokenizer(X_val.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt',)

input_ids_train_roberta = torch.cat([train_encodings_roberta['input_ids']], dim=0)
attention_mask_train_roberta = torch.cat([train_encodings_roberta['attention_mask']], dim=0)
labels_train = torch.tensor(y_train.to_list())

input_ids_val_roberta = torch.cat([val_encodings_roberta['input_ids']], dim=0)
attention_mask_val_roberta = torch.cat([val_encodings_roberta['attention_mask']], dim=0)
labels_val = torch.tensor(y_val.to_list())

train_tensor_roberta = TensorDataset(input_ids_train_roberta,attention_mask_train_roberta,labels_train)
val_tensor_roberta = TensorDataset(input_ids_val_roberta,attention_mask_val_roberta,labels_val)

# https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification
batch_size = 16
train_dataloader_roberta = DataLoader(
            train_tensor_roberta,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )
val_dataloader_roberta = DataLoader(
            val_tensor_roberta,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_roberta =RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels = 2,)
model_roberta = model_roberta.to(device)

optimizer_bert = AdamW(model_roberta.parameters(),lr = 2e-5, eps = 1e-8 )

def validate(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0

  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

  accuracy = correct / total
  loss /= len(dataloader)
  return accuracy, loss

n_epoch = 10
best_val_loss = 0
for epoch in range(n_epoch):
  ts = time.time()

  for data in train_dataloader_roberta:
    input_ids,attention_masks,labels = data

    _input_ids = input_ids.to(device)
    _attention_masks = attention_masks.to(device)
    _labels = labels.to(device)

    outputs = model_roberta(_input_ids,token_type_ids=None,attention_mask=_attention_masks,labels=_labels)
    loss = outputs.loss
    loss.backward()
    optimizer_bert.step()
    optimizer_bert.zero_grad()
    train_loss = loss.item()

  val_accuracy, val_loss = validate(model_roberta, val_dataloader_roberta)
  te = time.time() - ts
  if val_loss > best_val_loss:
    best_val_loss = val_loss
    torch.save(model_roberta.state_dict(), "/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_roberta.pt")
  print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}, duration: {:.2f} s'.format(epoch+1, n_epoch, train_loss, val_loss, val_accuracy, te))

"""# Training XLM-ROBERTA-Base"""

xlm_base_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True,add_special_tokens=True)

train_encodings_xlm = xlm_base_tokenizer(X_train.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')
val_encodings_xlm = xlm_base_tokenizer(X_val.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt',)

input_ids_train_xlm = torch.cat([train_encodings_xlm['input_ids']], dim=0)
attention_mask_train_xlm = torch.cat([train_encodings_xlm['attention_mask']], dim=0)
labels_train = torch.tensor(y_train.to_list())

input_ids_val_xlm = torch.cat([val_encodings_xlm['input_ids']], dim=0)
attention_mask_val_xlm = torch.cat([val_encodings_xlm['attention_mask']], dim=0)
labels_val = torch.tensor(y_val.to_list())

train_tensor_xlm = TensorDataset(input_ids_train_xlm,attention_mask_train_xlm,labels_train)
val_tensor_xlm = TensorDataset(input_ids_val_xlm,attention_mask_val_xlm,labels_val)

# https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification
batch_size = 16
train_dataloader_xlm = DataLoader(
            train_tensor_xlm,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )
val_dataloader_xlm = DataLoader(
            val_tensor_xlm,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_xlm = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',num_labels = 2,)

model_xlm = model_xlm.to(device)

optimizer_xlm = torch.optim.SGD(model_xlm.parameters(),lr = 2e-5,weight_decay=0.5)

def validate(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0

  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

  accuracy = correct / total
  loss /= len(dataloader)
  return accuracy, loss

n_epoch = 30
best_val_loss = 0
for epoch in range(n_epoch):
  ts = time.time()

  for data in train_dataloader_xlm:
    input_ids,attention_masks,labels = data

    _input_ids = input_ids.to(device)
    _attention_masks = attention_masks.to(device)
    _labels = labels.to(device)

    outputs = model_xlm(_input_ids,token_type_ids=None,attention_mask=_attention_masks,labels=_labels)
    loss = outputs.loss
    loss.backward()
    optimizer_xlm.step()
    optimizer_xlm.zero_grad()
    train_loss = loss.item()

  val_accuracy, val_loss = validate(model_xlm, val_dataloader_xlm)
  te = time.time() - ts
  if val_loss > best_val_loss:
    best_val_loss = val_loss
    torch.save(model_xlm.state_dict(), "/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_xlm.pt")
  print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}, duration: {:.2f} s'.format(epoch+1, n_epoch, train_loss, val_loss, val_accuracy, te))

##################test_final_nlp.py##############################
# -*- coding: utf-8 -*-
"""test_final_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y_f0iocmC_T3eoOXdaxqN0YdKgDQasSQ
"""

!pip install transformers
!pip install datasets

"""# Importing Libraries"""

import pandas as pd
import numpy as np
import json
from collections import Counter
from datasets import load_dataset
import re
import spacy
import time
import joblib
import matplotlib.pyplot as plt


import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix , accuracy_score, roc_curve ,auc
from nltk.tokenize import TreebankWordTokenizer, word_tokenize

from transformers import BertTokenizerFast, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, XLMRobertaForSequenceClassification,RobertaTokenizer, AdamW, get_linear_schedule_with_warmup,AutoTokenizer

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split
import pickle

spacy.prefer_gpu()
nlp = spacy.load("en_core_web_sm")
nltk.download('punkt')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""# Loading and Preprocessing Dataset"""

test = load_dataset('squad_v2',split='validation')

test_df = pd.DataFrame(test)
test_df

label=[]
for r in test_df['answers']:
  if r['text'] == []:
    label.append(1)
  else:
    label.append(0)

test_df['label']=label
test_df['label'].value_counts()

test_df['feature'] = test_df['context'] + ' ' + test_df['question']
test_df['feature'][0]

sample_df = test_df.sample(frac=1, random_state=42)  
sample_df = sample_df.head(500)
sample_df = sample_df.reset_index(drop=True)

X_test = sample_df['feature']

"""# Evaluating Logistic Regression"""

#https://stackoverflow.com/questions/53664775/how-to-remove-punctuation-in-python
sample_df['proccessed_context'] = sample_df['feature'].apply(lambda x: re.sub(r'http\S+', '', x))
# remove punctuation marks
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'
sample_df['proccessed_context']= sample_df['proccessed_context'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))
# convert text to lowercase
sample_df['proccessed_context'] = sample_df['proccessed_context'].str.lower()
#https://iconix.github.io/portfolio%20building/2017/09/25/nlp-for-tasks
#https://stackoverflow.com/questions/54849111/negation-and-dependency-parsing-with-spacy
def extract_pos(text):
    doc = nlp(text)
    pos_tags = [token.pos_ for token in doc]
    return pos_tags 


def extract_neg(text):
    doc = nlp(text)
    negations =np.zeros(len(doc))

    for i, token in enumerate(doc):
        if token.dep_ == "neg":
            # Check if negation applies to target word or its children
            target = None
            for child in token.children:
                if child.dep_ in ["acomp", "amod", "advcl", "advmod", "attr", "ccomp", "conj", "dobj", "iobj", "mark", "nsubj", "nsubjpass", "oprd", "xcomp"]:
                    target = child
                    break
            if target is None:
                target = token.head
            if target is not None:
                negations[target.i] = 1
                for child in target.children:
                    if child.dep_ in ["acomp", "amod", "advcl", "advmod", "attr", "ccomp", "conj", "dobj", "iobj", "mark", "nsubj", "nsubjpass", "oprd", "xcomp"]:
                        negations[child.i] = 1

    return negations
tokenizer_tb = TreebankWordTokenizer()
sample_df['tokenized_context'] = sample_df['proccessed_context'].apply(lambda x : tokenizer_tb.tokenize(x))

features_pos = sample_df['proccessed_context'].apply(lambda x : extract_pos(x))
features_neg = sample_df['proccessed_context'].apply(lambda x : extract_neg(x))

vectorizer = TfidfVectorizer()

X_context = vectorizer.fit_transform(sample_df['proccessed_context'])
_X_context = vectorizer.fit_transform([' '.join(tokens) for tokens in sample_df['tokenized_context']])
X_postag = vectorizer.fit_transform([' '.join(tokens) for tokens in features_pos])
sum_up= np.array(features_neg)
neg_sum = []
for val in sum_up:
  neg_sum.append(val.sum())
neg_array = np.array(neg_sum)
X_features = np.concatenate((_X_context.toarray(),X_postag.toarray(),neg_array.reshape(-1,1)),axis=1)

X_val_set = np.array(X_features)

# #https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/
# filename = '/content/drive/MyDrive/NLP-COURSEWORK/models/baselinemodel.h5'
# baseline_lr = pickle.load(open(filename, 'rb'))

# baseline_lr.predict(X_features)

# # Logistic regression failed to validate the test set ie dev set of SQUAD, so to evaluate loading the validation set splited from train in .npy format
# X_test_lr = np.load('/content/drive/MyDrive/NLP-COURSEWORK/dataset/X_test.npy')
# y_test_lr = np.load('/content/drive/MyDrive/NLP-COURSEWORK/dataset/y_test.npy')

# baseline_lr.predict(X_test_lr)

"""### Evaluation of logistic regresion is done in training.ipynb

# Evaluating BERT-Base
"""

bert_base_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased", do_lower_case=True,add_special_tokens=True)

test_encodings_bert = bert_base_tokenizer(X_test.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')

input_ids_test_bert = torch.cat([test_encodings_bert['input_ids']], dim=0)
attention_mask_test_bert = torch.cat([test_encodings_bert['attention_mask']], dim=0)
labels_test = torch.tensor(sample_df['label'].to_list())

input_ids_test_bert = input_ids_test_bert.to(device)
attention_mask_test_bert = attention_mask_test_bert.to(device)
labels_test = labels_test.to(device)

test_tensor_bert = TensorDataset(input_ids_test_bert,attention_mask_test_bert,labels_test)
batch_size = 16
test_dataloader_bert = DataLoader(
            test_tensor_bert,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_bert = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", # Use the 12-layer BERT model, with an uncased vocab.
    num_labels = 2, # The number of output labels--2 for binary classification.
)

model_bert = model_bert.to(device)

model_bert.load_state_dict(torch.load("/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_bert.pt"))

model_bert.eval()

def test(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0
  _predicted = []
  _pred_prob= []
  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        # Calculate probabilities
        probs = F.softmax(outputs.logits, dim=1)
        _pred_prob.append(probs)
        _predicted.append(predicted)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
  accuracy = correct / total
  loss /= len(dataloader)
  return _predicted, accuracy, loss,_pred_prob

predicted, accuracy, loss, pred_prob = test(model_bert,test_dataloader_bert)
_pred_prob_bert = torch.cat(pred_prob, dim=0)
pred_prob_bert = _pred_prob_bert[:,0]

print('Test Accuracy of BERT-Base: ', accuracy)

pred_bert = torch.cat(predicted, dim=0)
print(classification_report(labels_test.cpu(), pred_bert.cpu(),target_names=['answerable','unanswerable']))
#https://www.turing.com/kb/how-to-plot-confusion-matrix
# Calculate confusion matrix
cm01 = confusion_matrix(labels_test.cpu(), pred_bert.cpu())
# Plot confusion matrix
plt.imshow(cm01, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['answerable','unanswerable'], rotation=45)
plt.yticks(tick_marks, ['answerable','unanswerable'])
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Add numbers to the plot
thresh = cm01.max() / 2.
for i, j in np.ndindex(cm01.shape):
    plt.text(j, i, format(cm01[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm01[i, j] > thresh else "black")

plt.show()

accuracy

"""# Evaluating ROBERTA-Base"""

roberta_base_tokenizer = RobertaTokenizer.from_pretrained("roberta-base", do_lower_case=True,add_special_tokens=True)

test_encodings_roberta = roberta_base_tokenizer(X_test.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')

input_ids_test_roberta = torch.cat([test_encodings_roberta['input_ids']], dim=0)
attention_mask_test_roberta = torch.cat([test_encodings_roberta['attention_mask']], dim=0)
labels_test = torch.tensor(sample_df['label'].to_list())

input_ids_test_roberta = input_ids_test_roberta.to(device)
attention_mask_test_roberta = attention_mask_test_roberta.to(device)
labels_test = labels_test.to(device)

test_tensor_roberta = TensorDataset(input_ids_test_roberta,attention_mask_test_roberta,labels_test)
batch_size = 16
test_dataloader_roberta = DataLoader(
            test_tensor_roberta,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_roberta =RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels = 2,)
model_roberta = model_roberta.to(device)

model_roberta.load_state_dict(torch.load("/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_roberta.pt"))

model_roberta.eval()

import torch.nn.functional as F

def test(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0
  _predicted = []
  _pred_prob= []
  sample_text=[]
  sample_predicted_labels = []
  sample_true_labels = []

  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        decoded_tokens = roberta_base_tokenizer.decode(input_ids[0],skip_special_tokens=True)
        sample_text.append(decoded_tokens)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        sample_true_labels.append(labels[0].cpu())
        sample_predicted_labels.append(predicted[0].cpu())

        # Calculate probabilities
        probs = F.softmax(outputs.logits, dim=1)
        _pred_prob.append(probs)
        _predicted.append(predicted)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
  accuracy = correct / total
  loss /= len(dataloader)
  return _predicted, accuracy, loss,_pred_prob,sample_true_labels,sample_predicted_labels,sample_text

predicted, accuracy, loss, pred_prob,sample_true_labels,sample_predicted_labels,sample_text = test(model_roberta,test_dataloader_roberta)
_pred_prob_roberta = torch.cat(pred_prob, dim=0)
pred_prob_roberta = _pred_prob_roberta[:,0]

df_sample =  pd.DataFrame(sample_text,columns=['text'])

df_sample['True_label'] = sample_true_labels
df_sample['Predicted_label'] = sample_predicted_labels

df_sample['text'][26]

df_sample['text'][27]

print('Test Accuracy of ROBERTA: ', accuracy)

pred_roberta = torch.cat(predicted, dim=0)
print(classification_report(labels_test.cpu(), pred_roberta.cpu(),target_names=['answerable','unanswerable']))
#https://www.turing.com/kb/how-to-plot-confusion-matrix
# Calculate confusion matrix
cm01 = confusion_matrix(labels_test.cpu(), pred_roberta.cpu())
# Plot confusion matrix
plt.imshow(cm01, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['answerable','unanswerable'], rotation=45)
plt.yticks(tick_marks, ['answerable','unanswerable'])
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Add numbers to the plot
thresh = cm01.max() / 2.
for i, j in np.ndindex(cm01.shape):
    plt.text(j, i, format(cm01[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm01[i, j] > thresh else "black")

plt.show()

"""# Evaluating XLM-ROBERTA-Base"""

xlm_base_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True,add_special_tokens=True)

test_encodings_xlm = xlm_base_tokenizer(X_test.to_list(), truncation=True, padding=True, max_length=512, return_tensors = 'pt')

input_ids_test_xlm = torch.cat([test_encodings_xlm['input_ids']], dim=0)
attention_mask_test_xlm = torch.cat([test_encodings_xlm['attention_mask']], dim=0)
labels_test = torch.tensor(sample_df['label'].to_list())

input_ids_test_xlm = input_ids_test_xlm.to(device)
attention_mask_test_xlm = attention_mask_test_xlm.to(device)
labels_test = labels_test.to(device)

test_tensor_xlm = TensorDataset(input_ids_test_xlm,attention_mask_test_xlm,labels_test)
batch_size = 16
test_dataloader_xlm = DataLoader(
            test_tensor_xlm,  # The training samples.
            batch_size = batch_size # Trains with this batch size.
        )

model_xlm = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',num_labels = 2,)
model_xlm = model_xlm.to(device)

model_xlm.load_state_dict(torch.load("/content/drive/MyDrive/NLP-COURSEWORK/models/best_model_xlm.pt"))

model_xlm.eval()

def test(model, dataloader):
  model.eval()
  correct = 0
  total = 0
  loss = 0
  _predicted = []
  _pred_prob= []

  with torch.no_grad():
    for data in dataloader:
        input_ids, attention_masks, labels = data
        input_ids = input_ids.to(device)
        attention_masks = attention_masks.to(device)
        labels = labels.to(device)
        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)
        loss += outputs.loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        # Calculate probabilities
        probs = F.softmax(outputs.logits, dim=1)
        _pred_prob.append(probs)
        _predicted.append(predicted)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
  accuracy = correct / total
  loss /= len(dataloader)
  return _predicted, accuracy, loss,_pred_prob

predicted, accuracy, loss,pred_prob = test(model_xlm,test_dataloader_xlm)
_pred_prob_xlm = torch.cat(pred_prob, dim=0)
pred_prob_xlm = _pred_prob_xlm[:,0]

print('Test Accuracy of XLM-ROBERTA', accuracy)

pred_xlm = torch.cat(predicted, dim=0)
print(classification_report(labels_test.cpu(), pred_xlm.cpu(),target_names=['answerable','unanswerable']))
#https://www.turing.com/kb/how-to-plot-confusion-matrix
# Calculate confusion matrix
cm01 = confusion_matrix(labels_test.cpu(), pred_xlm.cpu())
# Plot confusion matrix
plt.imshow(cm01, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['answerable','unanswerable'], rotation=45)
plt.yticks(tick_marks, ['answerable','unanswerable'])
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')

# Add numbers to the plot
thresh = cm01.max() / 2.
for i, j in np.ndindex(cm01.shape):
    plt.text(j, i, format(cm01[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm01[i, j] > thresh else "black")

plt.show()

bert_fpr, bert_tpr, bert_thresholds = roc_curve(labels_test.cpu(),pred_prob_bert.cpu())
auc1 = auc(bert_fpr, bert_tpr)

roberta_fpr, roberta_tpr, roberta_thresholds = roc_curve(labels_test.cpu(),pred_prob_roberta.cpu())
auc2 = auc(roberta_fpr, roberta_tpr)

xlm_fpr, xlm_tpr, xlm_thresholds = roc_curve(labels_test.cpu(),pred_prob_xlm.cpu())
auc3 = auc(xlm_fpr,xlm_tpr)

plt.plot(bert_fpr, bert_tpr, lw=2, label='BERT-base, Auc='+str(round(auc1,2)))
plt.plot(roberta_fpr, roberta_tpr, lw=2, label='ROBERTA-base, Auc='+str(round(auc2,2)))
plt.plot(xlm_fpr, xlm_tpr, lw=2, label='XLM-ROBERTA-base, Auc='+str(round(auc3,2)))

plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.legend()

